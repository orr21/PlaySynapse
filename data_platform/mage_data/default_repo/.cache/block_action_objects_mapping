{"block_file": {"data_exporters/save_pbp_to_minio_gold.py:data_exporter:python:save pbp to minio gold": {"content": "import os\nimport polars as pl\nfrom deltalake import DeltaTable  # Required for the delete operation\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_to_delta(df, **kwargs):\n    if df.height == 0: return\n\n    # 1. Clean nulls and get unique IDs\n    game_ids = df[\"game_id\"].drop_nulls().unique().to_list()\n    \n    if not game_ids:\n        print(\"No valid game_ids found to process.\")\n        return\n\n    # 2. Format Game IDs for the SQL predicate\n    # Ensure IDs are strings if they need to be, or numbers otherwise\n    is_numeric = df[\"game_id\"].dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt32, pl.UInt64]\n    \n    if is_numeric:\n        # e.g., game_id in (123, 456)\n        ids_list_str = \", \".join([str(gid) for gid in game_ids])\n        predicate = f\"game_id in ({ids_list_str})\"\n    else:\n        # e.g., game_id in ('123', '456')\n        ids_list_str = \", \".join([f\"'{gid}'\" for gid in game_ids])\n        predicate = f\"game_id in ({ids_list_str})\"\n\n    table_path = \"s3://gold/nba/pbp/\"\n    \n    storage_options = {\n        \"AWS_ENDPOINT_URL\": os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        \"AWS_ACCESS_KEY_ID\": os.getenv('MINIO_ROOT_USER', 'admin'),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        \"AWS_REGION\": \"us-east-1\",\n        \"AWS_ALLOW_HTTP\": \"true\",\n        \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n    }\n\n    # 3. DELETE existing records for these games\n    try:\n        dt = DeltaTable(table_path, storage_options=storage_options)\n        dt.delete(predicate)\n        print(f\"\ud83d\uddd1\ufe0f Deleted old data for games: {ids_list_str}\")\n    except Exception as e:\n        # If table doesn't exist yet, we can skip delete and just append\n        print(f\"\u26a0\ufe0f Table might not exist or delete failed (safe to ignore on first run): {e}\")\n\n    # 4. APPEND the new corrected data\n    # We use 'append' because we just deleted the old versions.\n    # 'overwrite' would wipe the whole table again.\n    delta_write_options = {\n        \"partition_by\": [\"season\", \"game_date\"],\n        \"schema_mode\": \"merge\" # merge schema in case new columns appeared\n    }\n\n    df.write_delta(\n        table_path,\n        mode=\"append\",  # Crucial: Append, don't Overwrite\n        storage_options=storage_options,\n        delta_write_options=delta_write_options\n    )\n    \n    print(f\"\u2705 PBP Gold updated (Delete+Append) for games: {ids_list_str}\")", "file_path": "data_exporters/save_pbp_to_minio_gold.py", "language": "python", "type": "data_exporter", "uuid": "save_pbp_to_minio_gold"}, "data_exporters/save_players_to_minio_bronze.py:data_exporter:python:save players to minio bronze": {"content": "import boto3\nimport json\nimport os\nimport polars as pl\nfrom datetime import datetime\n\n@data_exporter\ndef export_to_bronze(df, **kwargs):\n    s3_client = boto3.client(\n        's3',\n        endpoint_url=os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        aws_access_key_id=os.getenv('MINIO_ROOT_USER', 'admin'),\n        aws_secret_access_key=os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        region_name='us-east-1' \n    )\n\n    records = df.to_dicts()\n\n    for record in records:\n\n        season = record['season']\n        ingested_at = record['ingested_at']\n\n        if isinstance(ingested_at, str):\n            ingested_at = datetime.fromisoformat(ingested_at)\n        \n        raw_json = json.loads(record['raw_content'])\n        \n        # 1. Carpeta por Fecha de Ingesti\u00f3n (para auditor\u00eda y Time Travel)\n        partition_path = f\"ingested_at={ingested_at.strftime('%Y-%m-%d')}\"\n        \n        # 2. Nombre del archivo con Semana Y Fecha de inicio\n        # Ejemplo: week_15_from_20241022.json\n        file_name = f\"{season}.json\"\n        \n        object_key = f\"nba/players/{partition_path}/{file_name}\"\n        \n        s3_client.put_object(\n            Bucket='bronze',\n            Key=object_key,\n            Body=json.dumps(raw_json),\n            ContentType='application/json'\n        )\n        \n        print(f\"\u2705 Archivo guardado: {object_key}\")", "file_path": "data_exporters/save_players_to_minio_bronze.py", "language": "python", "type": "data_exporter", "uuid": "save_players_to_minio_bronze"}, "data_exporters/save_pbp_to_minio_bronze.py:data_exporter:python:save pbp to minio bronze": {"content": "import boto3\nimport os\nimport polars as pl\n\n@data_exporter\ndef export_to_bronze(df, **kwargs):\n    if df.height == 0:\n        print(\"\u26a0\ufe0f No hay datos para exportar a Bronze.\")\n        return\n\n    s3_client = boto3.client(\n        's3',\n        endpoint_url=os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        aws_access_key_id=os.getenv('MINIO_ROOT_USER', 'admin'),\n        aws_secret_access_key=os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        region_name='us-east-1'\n    )\n\n    if isinstance(df['ingested_at'][0], str):\n        df = df.with_columns(\n            pl.col(\"ingested_at\").str.to_datetime()\n        )\n    \n    # Iteramos sobre el DataFrame\n    for row in df.to_dicts():\n        gid = row['game_id']\n        # Usamos el timestamp de ingesta para la ruta\n        # row['ingested_at'] es un objeto datetime\n        ts = row['ingested_at']\n        \n        # Estructura din\u00e1mica: nba/pbp/YYYY/MM/DD/ID_raw.json\n        key = (\n            f\"nba/pbp/\"\n            f\"{ts.year}/\"\n            f\"{ts.month:02d}/\"\n            f\"{ts.day:02d}/\"\n            f\"{gid}_raw.json\"\n        )\n\n        print(key)\n        \n        s3_client.put_object(\n            Bucket='bronze',\n            Key=key,\n            Body=row['raw_content'],\n            ContentType='application/json'\n        )\n    \n    print(f\"\u2705 {df.height} partidos guardados en Bronze con rutas din\u00e1micas.\")", "file_path": "data_exporters/save_pbp_to_minio_bronze.py", "language": "python", "type": "data_exporter", "uuid": "save_pbp_to_minio_bronze"}, "data_exporters/save_teams_to_minio_bronze.py:data_exporter:python:save teams to minio bronze": {"content": "import boto3\nimport json\nimport os\nimport polars as pl\nfrom datetime import datetime\n\n@data_exporter\ndef export_to_bronze(df, **kwargs):\n    s3_client = boto3.client(\n        's3',\n        endpoint_url=os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        aws_access_key_id=os.getenv('MINIO_ROOT_USER', 'admin'),\n        aws_secret_access_key=os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        region_name='us-east-1' \n    )\n\n    records = df.to_dicts()\n\n    for record in records:\n\n        season = record['season']\n        ingested_at = record['ingested_at']\n\n        if isinstance(ingested_at, str):\n            ingested_at = datetime.fromisoformat(ingested_at)\n        \n        raw_json = json.loads(record['raw_content'])\n        \n        # 1. Carpeta por Fecha de Ingesti\u00f3n (para auditor\u00eda y Time Travel)\n        partition_path = f\"ingested_at={ingested_at.strftime('%Y-%m-%d')}\"\n        \n        # 2. Nombre del archivo con Semana Y Fecha de inicio\n        # Ejemplo: week_15_from_20241022.json\n        file_name = f\"{season}.json\"\n        \n        object_key = f\"nba/teams/{partition_path}/{file_name}\"\n        \n        s3_client.put_object(\n            Bucket='bronze',\n            Key=object_key,\n            Body=json.dumps(raw_json),\n            ContentType='application/json'\n        )\n        \n        print(f\"\u2705 Archivo guardado: {object_key}\")", "file_path": "data_exporters/save_teams_to_minio_bronze.py", "language": "python", "type": "data_exporter", "uuid": "save_teams_to_minio_bronze"}, "data_exporters/save_pbp_to_mino_silver.py:data_exporter:python:save pbp to mino silver": {"content": "import os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_to_delta(df, **kwargs):\n    if df.height == 0: return\n\n    # Predicado para idempotencia basado en los IDs del partido\n    game_ids = df[\"game_id\"].unique().to_list()\n    ids_string = \", \".join([f\"'{gid}'\" for gid in game_ids])\n    \n    storage_options = {\n        \"AWS_ENDPOINT_URL\": os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        \"AWS_ACCESS_KEY_ID\": os.getenv('MINIO_ROOT_USER', 'admin'),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        \"AWS_REGION\": \"us-east-1\",\n        \"AWS_ALLOW_HTTP\": \"true\",\n        \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n    }\n\n    # Definimos el orden de las carpetas: A\u00f1o -> Mes -> D\u00eda -> Partido\n    delta_write_options = {\n        \"partition_by\": [\"year\", \"month\", \"day\", \"game_id\"],\n        \"predicate\": f\"game_id IN ({ids_string})\",\n        \"schema_mode\": \"overwrite\"\n    }\n\n    df.write_delta(\n        \"s3://silver/nba/pbp/\",\n        mode=\"overwrite\",\n        storage_options=storage_options,\n        overwrite_schema=True,\n        delta_write_options=delta_write_options\n    )\n    print(f\"\u2705 PBP Silver organizado por Year/Month/Day para los juegos: {ids_string}\")", "file_path": "data_exporters/save_pbp_to_mino_silver.py", "language": "python", "type": "data_exporter", "uuid": "save_pbp_to_mino_silver"}, "data_exporters/save_teams_to_minio_silver.py:data_exporter:python:save teams to minio silver": {"content": "import os\nimport polars as pl\n\n@data_exporter\ndef export_players_to_delta_silver(df, **kwargs):\n    if df.height == 0:\n        return\n\n    # 1. Aseguramos que la columna 'season' existe (viene del transformer)\n    # Ya no necesitamos crear year/month si vamos a particionar por season\n    if \"season\" not in df.columns:\n        raise ValueError(\"La columna 'season' es necesaria para el particionamiento.\")\n\n    # 2. Obtenemos las temporadas \u00fanicas para el predicado\n    # Esto asegura que el overwrite solo afecte a las temporadas presentes en el DataFrame\n    seasons = df.select(\"season\").unique().to_series().to_list()\n    \n    # Construimos el predicado: (season = '2024-25') OR (season = '2025-26')\n    predicate = \" OR \".join([f\"season = '{s}'\" for s in seasons])\n\n    storage_options = {\n        \"AWS_ENDPOINT_URL\": os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        \"AWS_ACCESS_KEY_ID\": os.getenv('MINIO_ROOT_USER', 'admin'),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        \"AWS_REGION\": \"us-east-1\",\n        \"AWS_ALLOW_HTTP\": \"true\",\n        \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n    }\n\n    # 3. Escritura Delta particionada por Season\n    delta_write_options = {\n        \"partition_by\": [\"season\"],\n        \"predicate\": predicate,\n        \"schema_mode\": \"overwrite\"\n    }\n\n    try:\n        df.write_delta(\n            \"s3://silver/nba/teams/\",\n            mode=\"overwrite\", \n            storage_options=storage_options,\n            overwrite_schema=True,\n            delta_write_options=delta_write_options\n        )\n        print(f\"\u2705 Silver Teams actualizado por Season. Predicado: {predicate}\")\n    except Exception as e:\n        if \"partition\" in str(e).lower():\n            print(\"\u26a0\ufe0f Error de partici\u00f3n. Si antes particionabas por year/month, debes borrar s3://silver/nba/teams/ en MinIO para cambiar el esquema a 'season'.\")\n        raise e", "file_path": "data_exporters/save_teams_to_minio_silver.py", "language": "python", "type": "data_exporter", "uuid": "save_teams_to_minio_silver"}, "data_exporters/save_players_to_minio_silver.py:data_exporter:python:save players to minio silver": {"content": "import os\nimport polars as pl\n\n@data_exporter\ndef export_players_to_delta_silver(df, **kwargs):\n    if df.height == 0:\n        return\n\n    # 1. Aseguramos que la columna 'season' existe (viene del transformer)\n    # Ya no necesitamos crear year/month si vamos a particionar por season\n    if \"season\" not in df.columns:\n        raise ValueError(\"La columna 'season' es necesaria para el particionamiento.\")\n\n    # 2. Obtenemos las temporadas \u00fanicas para el predicado\n    # Esto asegura que el overwrite solo afecte a las temporadas presentes en el DataFrame\n    seasons = df.select(\"season\").unique().to_series().to_list()\n    \n    # Construimos el predicado: (season = '2024-25') OR (season = '2025-26')\n    predicate = \" OR \".join([f\"season = '{s}'\" for s in seasons])\n\n    storage_options = {\n        \"AWS_ENDPOINT_URL\": os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        \"AWS_ACCESS_KEY_ID\": os.getenv('MINIO_ROOT_USER', 'admin'),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        \"AWS_REGION\": \"us-east-1\",\n        \"AWS_ALLOW_HTTP\": \"true\",\n        \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n    }\n\n    # 3. Escritura Delta particionada por Season\n    delta_write_options = {\n        \"partition_by\": [\"season\"],\n        \"predicate\": predicate,\n        \"schema_mode\": \"overwrite\"\n    }\n\n    try:\n        df.write_delta(\n            \"s3://silver/nba/players/\",\n            mode=\"overwrite\", \n            storage_options=storage_options,\n            overwrite_schema=True,\n            delta_write_options=delta_write_options\n        )\n        print(f\"\u2705 Silver Players actualizado por Season. Predicado: {predicate}\")\n    except Exception as e:\n        if \"partition\" in str(e).lower():\n            print(\"\u26a0\ufe0f Error de partici\u00f3n. Si antes particionabas por year/month, debes borrar s3://silver/nba/players/ en MinIO para cambiar el esquema a 'season'.\")\n        raise e", "file_path": "data_exporters/save_players_to_minio_silver.py", "language": "python", "type": "data_exporter", "uuid": "save_players_to_minio_silver"}, "data_exporters/nba_real_time_to_bronze.py:data_exporter:python:nba real time to bronze": {"content": "from mage_ai.streaming.sinks.base_python import BasePythonSink\nfrom typing import Callable, Dict, List\nimport boto3\nfrom datetime import datetime\nimport json\n\nif 'streaming_sink' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_sink\n\n@streaming_sink\nclass BronzeNbaSink(BasePythonSink):\n    def init_client(self):\n        self.client = boto3.client(\n            's3',\n            aws_access_key_id='admin',\n            aws_secret_access_key='password123',\n            endpoint_url='http://minio:9000',\n            region_name='us-east-1'\n        )\n        self.bucket = 'bronze'\n        self.prefix = 'nba/streaming/pbp'\n    \n    def batch_write(self, messages: List[Dict]):\n        if not messages: return\n        \n        # Agrupamos por timestamp para el archivo\n        dt = datetime.now()\n        jsonl_content = \"\\n\".join([json.dumps(m) for m in messages])\n        \n        partition = dt.strftime('year=%Y/month=%m/day=%d')\n        file_name = dt.strftime('%H%M%S_%f.jsonl')\n        key = f\"{self.prefix}/{partition}/{file_name}\"\n\n        self.client.put_object(Body=jsonl_content.encode('utf-8'), Bucket=self.bucket, Key=key)", "file_path": "data_exporters/nba_real_time_to_bronze.py", "language": "python", "type": "data_exporter", "uuid": "nba_real_time_to_bronze"}, "data_exporters/export_to_gradio.py:data_exporter:python:export to gradio": {"content": "from mage_ai.streaming.sinks.base_python import BasePythonSink\nfrom kafka import KafkaProducer\nimport json\nimport os\nfrom typing import List, Dict\n\nif 'streaming_sink' not in globals():\n    from mage_ai.data_preparation.decorators import streaming_sink\n\n@streaming_sink\nclass RedpandaGoldSink(BasePythonSink):\n    def init_client(self):\n        self.topic = 'nba_gold_events'\n        self.producer = KafkaProducer(\n            bootstrap_servers=[os.getenv('REDPANDA_BOOTSTRAP', 'redpanda:9092')],\n            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n            acks=1\n        )\n\n    def batch_write(self, messages: List[Dict]):\n            if not messages: \n                print(\"No hay mensajes para escribir.\")\n                return\n            \n            print(f\"DEBUG: Intentando escribir {len(messages)} mensajes en Redpanda...\")\n            \n            for msg in messages:\n                try:\n                    self.producer.send(self.topic, value=msg)\n                except Exception as e:\n                    print(f\"ERROR enviando mensaje: {e}\")\n            \n            self.producer.flush()\n            print(\"DEBUG: Flush completado con \u00e9xito.\")\n\n    def close(self):\n        if hasattr(self, 'producer'): self.producer.close()", "file_path": "data_exporters/export_to_gradio.py", "language": "python", "type": "data_exporter", "uuid": "export_to_gradio"}, "data_loaders/load_historical_games_ids.py:data_loader:python:load historical games ids": {"content": "from default_repo.utils.connectors.basketball.league.nba import NbaConnector\nimport polars as pl\nimport time\n\n@data_loader\ndef load_all_ids(*args, **kwargs):\n    connector = NbaConnector()\n    seasons = ['2024-25', '2025-26']\n    all_dfs = []\n\n    for season in seasons:\n        data = connector.fetch_data(metric_type='season_games', season=season)\n        res = data['resultSets'][0]\n        \n        df_season = pl.DataFrame(res['rowSet'], schema=res['headers'], orient=\"row\")\n        \n        df_clean = df_season.select([\n            pl.col(\"GAME_ID\"),\n            pl.col(\"GAME_DATE\"),\n            pl.lit(season).alias(\"SEASON\") \n        ]).unique()\n        \n        all_dfs.append(df_clean)\n        time.sleep(1)\n\n    return pl.concat(all_dfs).unique(subset=[\"GAME_ID\"])", "file_path": "data_loaders/load_historical_games_ids.py", "language": "python", "type": "data_loader", "uuid": "load_historical_games_ids"}, "data_loaders/pbp_download_historical.py:data_loader:python:pbp download historical": {"content": "import requests\nimport json\nimport polars as pl\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\ndef fetch_single_game(game):\n    \"\"\"Descarga un solo juego (igual que antes)\"\"\"\n    gid = game['GAME_ID']\n    season = game.get('SEASON', 'N/A')\n    \n    # URL del CDN (m\u00e1s r\u00e1pido y estable que la API din\u00e1mica)\n    cdn_url = f\"https://cdn.nba.com/static/json/liveData/playbyplay/playbyplay_{gid}.json\"\n    \n    retry_strategy = Retry(\n        total=3,\n        backoff_factor=1,\n        status_forcelist=[429, 500, 502, 503, 504],\n        allowed_methods=[\"GET\"]\n    )\n    adapter = HTTPAdapter(max_retries=retry_strategy)\n    session = requests.Session()\n    session.mount(\"https://\", adapter)\n    \n    try:\n        # Timeout corto para conectar, largo para leer\n        response = session.get(cdn_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=(3, 10))\n        if response.status_code == 200:\n            return {\n                \"game_id\": gid,\n                \"season\": season,\n                \"ingested_at\": datetime.now().isoformat(),\n                \"raw_content\": json.dumps(response.json()) # Esto ocupa mucha RAM en Python\n            }\n    except Exception:\n        return None\n    finally:\n        session.close()\n    return None\n\n@data_loader\ndef download_pbp_chunked(df_games, *args, **kwargs):\n    game_list = df_games.to_dicts()\n    total_games = len(game_list)\n    \n    # CONFIGURACI\u00d3N\n    BATCH_SIZE = 50   \n    MAX_WORKERS = 8   \n    THREAD_TIMEOUT = 30  # Segundos m\u00e1ximos que permitimos a UN hilo trabajar\n    \n    dfs_chunks = []   \n\n    def chunked_list(lst, n):\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]\n\n    pbar = tqdm(total=total_games, desc=\"Descargando PBP\", unit=\"games\")\n\n    # Usamos el Context Manager del Executor fuera del loop para reutilizar hilos\n    # o dentro si queremos limpieza total. Para evitar \"hilos colgados\", \n    # lo manejaremos con cautela.\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        for batch in chunked_list(game_list, BATCH_SIZE):\n            batch_results = []\n            \n            # Mapeamos los futuros\n            future_to_game = {executor.submit(fetch_single_game, g): g for g in batch}\n            \n            try:\n                # La clave est\u00e1 aqu\u00ed: as_completed con timeout global del lote\n                # Si el lote tarda m\u00e1s de (TIMEOUT * BATCH), algo va mal\n                for future in as_completed(future_to_game, timeout=THREAD_TIMEOUT + 10):\n                    try:\n                        res = future.result() # Aqu\u00ed podr\u00edas poner otro timeout si quisieras\n                        if res:\n                            batch_results.append(res)\n                    except Exception as e:\n                        print(f\"\u274c Error en hilo: {e}\")\n                    finally:\n                        pbar.update(1)\n            \n            except TimeoutError:\n                print(f\"\\n\u26a0\ufe0f Timeout alcanzado en el lote. Saltando hilos colgados...\")\n                # Nota: Los hilos colgados seguir\u00e1n en segundo plano hasta que el proceso muera,\n                # pero el flujo de tu programa principal continuar\u00e1.\n\n            if batch_results:\n                dfs_chunks.append(pl.DataFrame(batch_results))\n                del batch_results \n            \n    pbar.close()\n\n    if not dfs_chunks:\n        return pl.DataFrame()\n\n    return pl.concat(dfs_chunks)", "file_path": "data_loaders/pbp_download_historical.py", "language": "python", "type": "data_loader", "uuid": "pbp_download_historical"}, "data_loaders/fetch_yesterday_games_pbp.py:data_loader:python:fetch yesterday games pbp": {"content": "import polars as pl\nimport requests\nimport json\nfrom datetime import date, timedelta\nimport os\nimport time\n\n@data_loader\ndef load_pbp_data(schedule_df, *args, **kwargs):\n    # Configuraci\u00f3n de conexi\u00f3n (MinIO/S3)\n    storage_options = {\n        \"AWS_ENDPOINT_URL\": os.getenv('MINIO_ENDPOINT', 'http://minio:9000'),\n        \"AWS_ACCESS_KEY_ID\": os.getenv('MINIO_ROOT_USER', 'admin'),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv('MINIO_ROOT_PASSWORD', 'password123'),\n        \"AWS_REGION\": \"us-east-1\",\n        \"AWS_ALLOW_HTTP\": \"true\",\n        \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n    }\n    \n    # 1. Definimos el rango de fechas (\u00daltimos 3 d\u00edas: ayer, anteayer y trasanteayer)\n    now = date.today()\n    last_3_days = [now - timedelta(days=i) for i in range(1, 4)]\n    \n    # 2. Calculamos la temporada actual (ej: '2024-25')\n    current_year = now.year\n    if now.month < 7:\n        season_str = f\"{current_year-1}-{str(current_year)[-2:]}\"\n    else:\n        season_str = f\"{current_year}-{str(current_year+1)[-2:]}\"\n    \n    target_games = schedule_df.with_columns(\n        pl.col(\"gamedate\").str.to_datetime(format=\"%m/%d/%Y %H:%M:%S\").dt.date()\n    ).filter(\n        (pl.col(\"gamedate\").is_in(last_3_days)) &\n        (pl.col(\"gamestatus\") == 3)\n    )\n\n    game_ids = target_games.get_column(\"gameid\").unique().to_list()\n    \n    if not game_ids:\n        print(f\"\ud83d\udced No hay partidos finalizados en el rango: {last_3_days}\")\n        return pl.DataFrame()\n\n    print(f\"\ud83c\udfc0 Se encontraron {len(game_ids)} partidos. Iniciando descarga...\")\n\n    all_pbp = []\n    for i, gid in enumerate(game_ids, 1):\n        url = f\"https://cdn.nba.com/static/json/liveData/playbyplay/playbyplay_{gid}.json\"\n        try:\n            res = requests.get(url, timeout=15)\n            if res.status_code == 200:\n                all_pbp.append({\n                    \"game_id\": gid,\n                    \"season\": season_str,\n                    \"raw_content\": json.dumps(res.json()),\n                    \"ingested_at\": now \n                })\n                print(f\"\u2705 [{i}/{len(game_ids)}] Game ID: {gid} OK\")\n            else:\n                print(f\"\u26a0\ufe0f [{i}/{len(game_ids)}] Error {res.status_code} en ID: {gid}\")\n            \n            time.sleep(0.5)\n            \n        except Exception as e:\n            print(f\"\u274c Error cr\u00edtico en {gid}: {e}\")\n\n    return pl.DataFrame(all_pbp)", "file_path": "data_loaders/fetch_yesterday_games_pbp.py", "language": "python", "type": "data_loader", "uuid": "fetch_yesterday_games_pbp"}, "data_loaders/fetch_players_raw.py:data_loader:python:fetch players raw": {"content": "from default_repo.utils.connectors.basketball.league.nba import NbaConnector\nimport polars as pl\nimport time\nfrom datetime import datetime, timedelta\nimport json\n\n@data_loader\ndef load_current_season_players(*args, **kwargs):\n    connector = NbaConnector()\n    \n    # 1. Calculamos la season actual din\u00e1micamente\n    now = datetime.now()\n    # Si estamos antes de octubre, la temporada actual es (A\u00f1oAnterior)-(A\u00f1oActual corto)\n    # Si estamos en octubre o despu\u00e9s, es (A\u00f1oActual)-(A\u00f1oSiguiente corto)\n    if now.month < 10:\n        year_start = now.year - 1\n        year_end = str(now.year)[2:]\n    else:\n        year_start = now.year\n        year_end = str(now.year + 1)[2:]\n    \n    current_season = f\"{year_start}-{year_end}\"\n    \n    all_season_responses = []\n\n    print(f\"\ud83d\udce5 Downloading Current Players Profile - Season: {current_season}\")\n    \n    # Hacemos la petici\u00f3n para la temporada calculada\n    json_response = connector.fetch_data(\n        metric_type='season_players', \n        season=current_season, \n        perMode='Totals' \n    )\n    \n    if json_response:\n        all_season_responses.append({\n            \"season\": current_season,\n            \"raw_content\": json.dumps(json_response),\n            \"ingested_at\": now.isoformat()\n        })\n            \n    return pl.DataFrame(all_season_responses)", "file_path": "data_loaders/fetch_players_raw.py", "language": "python", "type": "data_loader", "uuid": "fetch_players_raw"}, "data_loaders/load_historical_teams.py:data_loader:python:load historical teams": {"content": "from default_repo.utils.connectors.basketball.league.nba import NbaConnector\nimport polars as pl\nimport time\nfrom datetime import datetime, timedelta\nimport json\n\n@data_loader\ndef load_multi_season_teams(*args, **kwargs):\n    connector = NbaConnector()\n    # Definimos el rango de temporadas que quieres traer\n    seasons = ['2023-24', '2024-25']\n    \n    all_season_responses = []\n\n    for season in seasons:\n        print(f\"\ud83d\udce5 Downloading Teams Profile - Season: {season}\")\n        \n        # Hacemos la petici\u00f3n sin DateFrom/DateTo para obtener \n        # la ficha m\u00e1s actualizada del jugador en esa temporada.\n        json_response = connector.fetch_data(\n            metric_type='season_teams', \n            season=season, \n            perMode='Totals' \n        )\n        \n        if json_response:\n            # Guardamos el contenido indicando a qu\u00e9 temporada pertenece\n            all_season_responses.append({\n                \"season\": season,\n                \"raw_content\": json.dumps(json_response),\n                \"ingested_at\": datetime.now().isoformat()\n            })\n        \n        # Respetamos el l\u00edmite de la API para no ser bloqueados\n        time.sleep(1.5)\n            \n    return pl.DataFrame(all_season_responses)", "file_path": "data_loaders/load_historical_teams.py", "language": "python", "type": "data_loader", "uuid": "load_historical_teams"}, "data_loaders/fetch_teams_raw.py:data_loader:python:fetch teams raw": {"content": "from default_repo.utils.connectors.basketball.league.nba import NbaConnector\nimport polars as pl\nimport time\nfrom datetime import datetime, timedelta\nimport json\n\n@data_loader\ndef load_current_season_teams(*args, **kwargs):\n    connector = NbaConnector()\n    \n    # 1. Calculamos la season actual din\u00e1micamente\n    now = datetime.now()\n    # Si estamos antes de octubre, la temporada actual es (A\u00f1oAnterior)-(A\u00f1oActual corto)\n    # Si estamos en octubre o despu\u00e9s, es (A\u00f1oActual)-(A\u00f1oSiguiente corto)\n    if now.month < 10:\n        year_start = now.year - 1\n        year_end = str(now.year)[2:]\n    else:\n        year_start = now.year\n        year_end = str(now.year + 1)[2:]\n    \n    current_season = f\"{year_start}-{year_end}\"\n    \n    all_season_responses = []\n\n    print(f\"\ud83d\udce5 Downloading Current Teams Profile - Season: {current_season}\")\n    \n    # Hacemos la petici\u00f3n para la temporada calculada\n    json_response = connector.fetch_data(\n        metric_type='season_teams', \n        season=current_season, \n        perMode='Totals' \n    )\n    \n    if json_response:\n        all_season_responses.append({\n            \"season\": current_season,\n            \"raw_content\": json.dumps(json_response),\n            \"ingested_at\": now.isoformat()\n        })\n            \n    return pl.DataFrame(all_season_responses)", "file_path": "data_loaders/fetch_teams_raw.py", "language": "python", "type": "data_loader", "uuid": "fetch_teams_raw"}, "data_loaders/nba_real_time_listener.yaml:data_loader:yaml:nba real time listener": {"content": "connector_type: kafka\nbootstrap_server: \"redpanda:9092\"\ntopic: nba_live\nconsumer_group: nba_events\ninclude_metadata: true         # CAMBIO: Un DE siempre quiere metadatos (offset, timestamp)\napi_version: \"2.8.0\"\nauto_offset_reset: earliest    # O 'latest' si no te importan los datos hist\u00f3ricos al reiniciar\nbatch_size: 5               # CAMBIO: Un poco m\u00e1s alto para eficiencia en r\u00e1fagas\ntimeout_ms: 100                # CAMBIO: Latencia sub-segundo para que sea \"Live\"\n\n# Uncomment the config below to use SSL config\n# security_protocol: \"SSL\"\n# ssl_config:\n#   cafile: \"CARoot.pem\"\n#   certfile: \"certificate.pem\"\n#   keyfile: \"key.pem\"\n#   password: password\n#   check_hostname: true\n\n# Uncomment the config below to use SASL_SSL config\n# security_protocol: \"SASL_SSL\"\n# sasl_config:\n#   mechanism: \"PLAIN\"\n#   username: username\n#   password: password\n\n# Uncomment the config below to use protobuf schema to deserialize message\n# serde_config:\n#   serialization_method: PROTOBUF\n#   schema_classpath: \"path.to.schema.SchemaClass\"\n", "file_path": "data_loaders/nba_real_time_listener.yaml", "language": "yaml", "type": "data_loader", "uuid": "nba_real_time_listener"}, "data_loaders/updated_schedule.py:data_loader:python:updated schedule": {"content": "import polars as pl\nimport requests\nimport json\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef load_and_transform_nba_schedule(*args, **kwargs):\n    # 1. Extraction\n    url = \"https://cdn.nba.com/static/json/staticData/scheduleLeagueV2.json\"\n    try:\n        response = requests.get(url, timeout=15)\n        response.raise_for_status()\n        data = response.json()\n    except Exception as e:\n        print(f\"\u274c Failed to fetch schedule: {e}\")\n        return pl.DataFrame()\n\n    # 2. Initial Processing & Flattening\n    # We create a temporary DF to use Polars' powerful JSON/Struct expressions\n    df = (\n        pl.DataFrame({\"raw\": [json.dumps(data)]})\n        .with_columns(pl.col(\"raw\").str.json_decode())\n        .with_columns(\n            pl.col(\"raw\")\n            .struct.field(\"leagueSchedule\")\n            .struct.field(\"gameDates\")\n            .alias(\"game_dates_list\")\n        )\n        .explode(\"game_dates_list\")\n        .unnest(\"game_dates_list\")\n        .explode(\"games\")\n        .unnest(\"games\")\n        .drop(\"raw\")\n    )\n\n    # 3. Normalization\n    # Standardize column names to lowercase for SQL-friendly environments\n    df.columns = [c.lower() for c in df.columns]\n\n    # Handle Null/Unknown types to prevent schema evolution issues in Delta\n    cols_to_cast = []\n    for col_name, dtype in df.schema.items():\n        if \"Null\" in str(dtype):\n            cols_to_cast.append(pl.col(col_name).cast(pl.Utf8))\n\n    if cols_to_cast:\n        df = df.with_columns(cols_to_cast)\n\n    # Add ingestion metadata\n    df = df.with_columns(pl.lit(datetime.now()).alias(\"ingested_at\"))\n\n    print(f\"\u2705 Processed {df.height} games from schedule.\")\n    return df", "file_path": "data_loaders/updated_schedule.py", "language": "python", "type": "data_loader", "uuid": "updated_schedule"}, "data_loaders/load_historical_players.py:data_loader:python:load historical players": {"content": "from default_repo.utils.connectors.basketball.league.nba import NbaConnector\nimport polars as pl\nimport time\nfrom datetime import datetime, timedelta\nimport json\n\n@data_loader\ndef load_multi_season_players(*args, **kwargs):\n    connector = NbaConnector()\n    # Definimos el rango de temporadas que quieres traer\n    seasons = ['2023-24', '2024-25']\n    \n    all_season_responses = []\n\n    for season in seasons:\n        print(f\"\ud83d\udce5 Downloading Players Profile - Season: {season}\")\n        \n        # Hacemos la petici\u00f3n sin DateFrom/DateTo para obtener \n        # la ficha m\u00e1s actualizada del jugador en esa temporada.\n        json_response = connector.fetch_data(\n            metric_type='season_players', \n            season=season, \n            perMode='Totals' \n        )\n        \n        if json_response:\n            # Guardamos el contenido indicando a qu\u00e9 temporada pertenece\n            all_season_responses.append({\n                \"season\": season,\n                \"raw_content\": json.dumps(json_response),\n                \"ingested_at\": datetime.now().isoformat()\n            })\n        \n        # Respetamos el l\u00edmite de la API para no ser bloqueados\n        time.sleep(1.5)\n            \n    return pl.DataFrame(all_season_responses)", "file_path": "data_loaders/load_historical_players.py", "language": "python", "type": "data_loader", "uuid": "load_historical_players"}, "transformers/extract_unique_teams.py:transformer:python:extract unique teams": {"content": "import polars as pl\nfrom datetime import datetime\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef transform(df_bronze, *args, **kwargs):\n    # 1. Decodificaci\u00f3n del JSON\n    df_transformed = df_bronze.with_columns([\n        pl.col(\"raw_content\").str.json_decode()\n    ]).with_columns([\n        pl.col(\"raw_content\").struct.field(\"resultSets\").list.get(0).alias(\"result_set\")\n    ])\n\n    # 2. Explode (\u00a1IMPORTANTE!: A\u00f1adimos ingested_at a la selecci\u00f3n)\n    df_exploded = df_transformed.select([\n        \"ingested_at\",  # <--- Faltaba esta columna aqu\u00ed\n        pl.col(\"season\"), # <--- Mantenemos la temporada aqu\u00ed\n        pl.col(\"result_set\").struct.field(\"headers\").alias(\"headers\"),\n        pl.col(\"result_set\").struct.field(\"rowSet\").alias(\"rows\")\n    ]).explode(\"rows\")\n\n    # 3. Mapeo Din\u00e1mico TOTAL\n    headers_list = df_exploded[\"headers\"].to_list()[0]\n    \n    df_final = df_exploded.with_columns([\n        pl.col(\"rows\").list.get(i).alias(header.lower())\n        for i, header in enumerate(headers_list)\n    ])\n\n    # 4. Enriquecimiento y Limpieza\n    # Ahora 'ingested_at' s\u00ed est\u00e1 disponible en el DataFrame\n    df_final = df_final.with_columns([\n        pl.col(\"ingested_at\").str.to_datetime().dt.date().alias(\"ingested_date\"),\n        \n        pl.format(\"{} {}\", pl.col(\"teamcity\"), pl.col(\"teamname\")).alias(\"full_name\"),\n        pl.format(\"https://cdn.nba.com/logos/nba/{}/global/L/logo.svg\", pl.col(\"teamid\")).alias(\"logo_url\"),\n        pl.col(\"season\").cast(pl.Utf8)\n    ])\n\n    # Si no existe 'abbreviation', intentamos usar 'teamslug' o dejarla nula\n    if \"abbreviation\" not in df_final.columns:\n        if \"teamslug\" in df_final.columns:\n             df_final = df_final.with_columns(pl.col(\"teamslug\").alias(\"abbreviation\"))\n        else:\n             df_final = df_final.with_columns(pl.lit(None, dtype=pl.Utf8).alias(\"abbreviation\"))\n\n    # 5. Cumplimiento Estricto del Esquema\n    from default_repo.utils.schemas import TEAMS_SILVER_SCHEMA\n    \n    # Creamos columnas faltantes\n    for col_name, dtype in TEAMS_SILVER_SCHEMA.items():\n        if col_name not in df_final.columns:\n            df_final = df_final.with_columns(pl.lit(None, dtype=dtype).alias(col_name))\n    \n    # Casteamos y Seleccionamos\n    final_cols = []\n    for col_name, dtype in TEAMS_SILVER_SCHEMA.items():\n        final_cols.append(pl.col(col_name).cast(dtype, strict=False))\n        \n    df_final = df_final.select(final_cols).unique()\n\n    return df_final\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    from default_repo.utils.schemas import validate_schema, TEAMS_SILVER_SCHEMA, TEAMS_SILVER_CRITICAL\n    assert output is not None, 'The output is undefined'\n    \n    # Validamos esquema\n    validate_schema(output, TEAMS_SILVER_SCHEMA, TEAMS_SILVER_CRITICAL)", "file_path": "transformers/extract_unique_teams.py", "language": "python", "type": "transformer", "uuid": "extract_unique_teams"}, "transformers/clean_pbp_silver.py:transformer:python:clean pbp silver": {"content": "import polars as pl\nimport json\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef transform_pbp_batch_optimized(df_bronze, *args, **kwargs):\n    # Convertimos a lista de dicts para iterar r\u00e1pido en Python puro\n    raw_data = df_bronze.to_dicts()\n    total_games = len(raw_data)\n    \n    print(f\"\u2699\ufe0f Procesando {total_games} partidos con estrategia de lotes...\")\n\n    # Aumenta el BATCH_SIZE si tienes RAM de sobra (ej. 1000 o 2000)\n    # Si ves que va lento, b\u00e1jalo a 500.\n    BATCH_SIZE = 200 \n    dfs_chunks = [] \n\n    for i in range(0, total_games, BATCH_SIZE):\n        batch = raw_data[i : i + BATCH_SIZE]\n        batch_actions = []\n\n        for row in batch:\n            try:\n                # El json.loads de Python es muy r\u00e1pido\n                content = json.loads(row['raw_content'])\n            except (json.JSONDecodeError, TypeError):\n                continue\n            \n            actions = content.get('game', {}).get('actions', [])\n            if not actions: continue\n\n            # Inyecci\u00f3n de metadatos ligera\n            game_id = row['game_id']\n            season = row.get('season', 'N/A')\n            \n            for action in actions:\n                action['game_id'] = game_id\n                action['season'] = season\n                batch_actions.append(action)\n\n        # Convertir a Polars por lote es casi instant\u00e1neo\n        if batch_actions:\n            chunk_df = pl.DataFrame(batch_actions, infer_schema_length=None) \n            # infer_schema_length=None acelera la carga al no escanear todo estrictamente\n            \n            # Normalizaci\u00f3n r\u00e1pida\n            chunk_df.columns = [c.lower() for c in chunk_df.columns]\n            \n            # Casteo preventivo para evitar errores al unir\n            # A\u00f1ade aqu\u00ed otras columnas si fallan al unir\n            # Casteo preventivo para evitar errores al unir\n            # Definimos tipos base para las columnas clave\n            cols_to_cast = {\n                \"actionnumber\": pl.Int64, \"period\": pl.Int64, \"personid\": pl.Int64, \n                \"teamid\": pl.Int64, \"possession\": pl.Int64, \"ordernumber\": pl.Int64,\n                \"x\": pl.Float64, \"y\": pl.Float64, \"shotdistance\": pl.Float64,\n                \"reboundtotal\": pl.Int64, \"pointstotal\": pl.Int64, \"assisttotal\": pl.Int64,\n                \"turnovertotal\": pl.Int64, \"foulpersonaltotal\": pl.Int64,\n                \"assistpersonid\": pl.Int64, \"stealpersonid\": pl.Int64, \"blockpersonid\": pl.Int64,\n                \"officialid\": pl.Int64, \"jumpballwonpersonid\": pl.Int64, \"jumpballlostpersonid\": pl.Int64,\n                \"jumpballrecoverdpersonid\": pl.Int64, \"fouldrawnpersonid\": pl.Int64,\n                \"shotactionnumber\": pl.Int64, \"xlegacy\": pl.Int64, \"ylegacy\": pl.Int64,\n                \"isfieldgoal\": pl.Int64, \"istargetscorelastperiod\": pl.Int64,\n                \"rebounddefensivetotal\": pl.Int64, \"reboundoffensivetotal\": pl.Int64,\n                \"foultechnicaltotal\": pl.Int64\n            }\n            \n            # Garantizar que todas las columnas del esquema existan\n            # Si no vienen en el JSON (ej. jumpball en un lote sin saltos), las creamos como nulos\n            for col_name, dtype in cols_to_cast.items():\n                if col_name not in chunk_df.columns:\n                    chunk_df = chunk_df.with_columns(pl.lit(None, dtype=dtype).alias(col_name))\n            \n            # Casteo seguro de las que s\u00ed existen o acabamos de crear\n            chunk_df = chunk_df.with_columns([\n                pl.col(c).cast(t, strict=False) for c, t in cols_to_cast.items()\n            ])\n\n            dfs_chunks.append(chunk_df)\n            del batch_actions # Liberar RAM\n\n        print(f\"\ud83d\udce6 Lote {i}/{total_games} completado.\")\n\n    if not dfs_chunks:\n        return pl.DataFrame()\n\n    print(\"\ud83d\udd04 Uniendo lotes...\")\n    df_final = pl.concat(dfs_chunks, how=\"diagonal\")\n\n    if \"timeactual\" in df_final.columns:\n        df_final = df_final.with_columns(\n            pl.col(\"timeactual\")\n            .str.to_datetime(strict=False)\n            # Primero le decimos a Polars que estos datos vienen en UTC\n            .dt.replace_time_zone(\"UTC\")\n            # Luego los convertimos a la hora de Nueva York\n            .dt.convert_time_zone(\"America/New_York\")\n            .alias(\"action_timestamp\")\n        )\n\n    if \"action_timestamp\" in df_final.columns:\n        df_final = df_final.with_columns([\n            pl.col(\"action_timestamp\").dt.year().alias(\"year\"),\n            pl.col(\"action_timestamp\").dt.month().alias(\"month\"),\n            pl.col(\"action_timestamp\").dt.day().alias(\"day\")\n        ])\n    else:\n        print(\"\u26a0\ufe0f Advertencia: No se pudo generar action_timestamp. Rellenando particiones con nulos.\")\n        df_final = df_final.with_columns([\n            pl.lit(None, dtype=pl.Int32).alias(\"year\"),\n            pl.lit(None, dtype=pl.Int8).alias(\"month\"),\n            pl.lit(None, dtype=pl.Int8).alias(\"day\")\n        ])\n\n    print(f\"\u2705 Hecho. {df_final.height} filas generadas. Columnas: {df_final.columns}\")\n    return df_final\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    from default_repo.utils.schemas import validate_schema, PBP_SILVER_SCHEMA, PBP_SILVER_CRITICAL\n    assert output is not None, 'The output is undefined'\n    \n    # Validamos esquema\n    validate_schema(output, PBP_SILVER_SCHEMA, PBP_SILVER_CRITICAL)", "file_path": "transformers/clean_pbp_silver.py", "language": "python", "type": "transformer", "uuid": "clean_pbp_silver"}, "transformers/clean_players_silver.py:transformer:python:clean players silver": {"content": "import polars as pl\nfrom datetime import datetime\nfrom default_repo.utils.schemas import PLAYERS_SILVER_SCHEMA, PLAYERS_SILVER_CRITICAL\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef transform_to_weekly_scd2(df_bronze, *args, **kwargs):\n    # 1. Decodificaci\u00f3n del JSON y extracci\u00f3n de la temporada\n    # Asumimos que df_bronze tiene las columnas: [\"season\", \"raw_content\"]\n    df_transformed = df_bronze.with_columns([\n        pl.col(\"raw_content\").str.json_decode()\n    ]).with_columns([\n        pl.col(\"raw_content\").struct.field(\"resultSets\").list.get(0).alias(\"result_set\")\n    ])\n\n    # 2. Explode: Una fila por registro (jugador)\n    # Mantenemos la columna \"season\" durante el select\n    df_exploded = df_transformed.select([\n        pl.col(\"season\"), # <--- Mantenemos la temporada aqu\u00ed\n        pl.col(\"result_set\").struct.field(\"headers\").alias(\"headers\"),\n        pl.col(\"result_set\").struct.field(\"rowSet\").alias(\"rows\")\n    ]).explode(\"rows\")\n\n    # 3. Mapeo Din\u00e1mico\n    headers_list = df_exploded[\"headers\"].to_list()[0]\n    \n    df_final = df_exploded.with_columns([\n        pl.col(\"rows\").list.get(i).alias(header.lower())\n        for i, header in enumerate(headers_list)\n    ])\n\n    # 4. L\u00f3gica de Fechas y Limpieza Final\n    df_final = df_final.with_columns(\n        pl.lit(datetime.now().date()).alias(\"ingested_date\")\n    )\n    \n    # Garantizar cumplimiento estricto del PLAYERS_SILVER_SCHEMA\n    \n    # 1. Crear columnas faltantes como Nulos\n    for col_name, dtype in PLAYERS_SILVER_SCHEMA.items():\n        if col_name not in df_final.columns:\n            df_final = df_final.with_columns(pl.lit(None, dtype=dtype).alias(col_name))\n\n    # 2. Castear y Seleccionar\n    final_cols = []\n    for col_name, dtype in PLAYERS_SILVER_SCHEMA.items():\n        final_cols.append(pl.col(col_name).cast(dtype, strict=False))\n\n    return df_final.select(final_cols).unique()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    from default_repo.utils.schemas import validate_schema\n    assert output is not None, 'The output is undefined'\n    \n    # Validamos esquema\n    validate_schema(output, PLAYERS_SILVER_SCHEMA, PLAYERS_SILVER_CRITICAL)", "file_path": "transformers/clean_players_silver.py", "language": "python", "type": "transformer", "uuid": "clean_players_silver"}, "transformers/dashboard_pbp_gold.py:transformer:python:dashboard pbp gold": {"content": "import polars as pl\nfrom default_repo.utils.schemas import DASHBOARD_GOLD_SCHEMA, DASHBOARD_GOLD_OUTPUT_CRITICAL\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef transform_pbp_to_dashboard_schema(pbp_df: pl.DataFrame):\n    # -------------------------------------------------------------------------\n    # PASO 0: CALCULAR OPONENTES CON LOS DATOS CRUDOS (CR\u00cdTICO)\n    # -------------------------------------------------------------------------\n    # Lo hacemos aqu\u00ed antes de filtrar nada. Incluso si personid es 0, el teamtricode existe.\n    game_opponents = pbp_df.select([\"game_id\", \"teamtricode\"]) \\\n                           .filter(pl.col(\"teamtricode\").is_not_null() & (pl.col(\"teamtricode\") != \"\")) \\\n                           .group_by(\"game_id\") \\\n                           .agg(pl.col(\"teamtricode\").unique().alias(\"teams\"))\n\n    # -------------------------------------------------------------------------\n    # 1. LIMPIEZA Y PRE-C\u00c1LCULO\n    # -------------------------------------------------------------------------\n    df = pbp_df.with_columns([\n        pl.col(\"personid\").cast(pl.Int64),\n        pl.col(\"assistpersonid\").cast(pl.Int64).fill_null(0),\n        pl.col(\"actiontype\").str.to_lowercase().fill_null(\"\"),\n        pl.col(\"shotresult\").str.to_lowercase().fill_null(\"\"),\n        \n        # Game Date\n        pl.col(\"action_timestamp\").str.to_datetime().dt.date().alias(\"game_date\") \n        if pbp_df.schema.get(\"action_timestamp\") == pl.Utf8 \n        else pl.col(\"action_timestamp\").dt.date().alias(\"game_date\")\n    ]).with_columns(\n        # Pre-c\u00e1lculo de puntos\n        pl.when((pl.col(\"shotresult\") == \"made\") & (pl.col(\"actiontype\").str.contains(\"3pt\"))).then(3)\n          .when((pl.col(\"shotresult\") == \"made\") & (pl.col(\"actiontype\").str.contains(\"2pt\"))).then(2)\n          .when((pl.col(\"shotresult\") == \"made\") & (pl.col(\"actiontype\").str.contains(\"free\"))).then(1)\n          .otherwise(0)\n          .alias(\"points_value\")\n    )\n\n    # -------------------------------------------------------------------------\n    # 2. AGRUPACI\u00d3N PRINCIPAL (FILTRANDO PERSONID != 0)\n    # -------------------------------------------------------------------------\n    # Ahora s\u00ed podemos filtrar, porque ya guardamos los oponentes en 'game_opponents'\n    stats_main = df.filter(pl.col(\"personid\") != 0).group_by([\"game_id\", \"personid\", \"playername\", \"teamtricode\"]).agg([\n        pl.col(\"points_value\").sum().cast(pl.Int64).alias(\"pts\"),\n        pl.col(\"actionnumber\").filter(pl.col(\"actiontype\").str.contains(\"rebound\")).count().cast(pl.Int64).alias(\"reb\"),\n        pl.col(\"actionnumber\").filter(pl.col(\"actiontype\").str.contains(\"pt\")).count().cast(pl.Int64).alias(\"fga\"),\n        pl.col(\"actionnumber\").filter((pl.col(\"actiontype\").str.contains(\"pt\")) & (pl.col(\"shotresult\") == \"made\")).count().cast(pl.Int64).alias(\"fgm\"),\n        pl.col(\"actionnumber\").filter(pl.col(\"actiontype\").str.contains(\"3pt\")).count().cast(pl.Int64).alias(\"3pa\"),\n        pl.col(\"actionnumber\").filter((pl.col(\"actiontype\").str.contains(\"3pt\")) & (pl.col(\"shotresult\") == \"made\")).count().cast(pl.Int64).alias(\"3pm\"),\n        pl.col(\"actionnumber\").filter(pl.col(\"actiontype\").str.contains(\"free\")).count().cast(pl.Int64).alias(\"fta\"),\n        pl.col(\"actionnumber\").filter((pl.col(\"actiontype\").str.contains(\"free\")) & (pl.col(\"shotresult\") == \"made\")).count().cast(pl.Int64).alias(\"ftm\"),\n        pl.col(\"turnovertotal\").sum().fill_null(0).cast(pl.Int64).alias(\"tov\"),\n        pl.col(\"stealpersonid\").is_not_null().sum().cast(pl.Int64).alias(\"stl\"),\n        pl.col(\"blockpersonid\").is_not_null().sum().cast(pl.Int64).alias(\"blk\")\n    ])\n\n    # 3. ASISTENCIAS\n    stats_ast = df.filter(pl.col(\"assistpersonid\") != 0) \\\n                  .group_by([\"game_id\", \"assistpersonid\"]) \\\n                  .agg(pl.len().cast(pl.Int64).alias(\"ast\")) \\\n                  .rename({\"assistpersonid\": \"personid\"})\n\n    # 4. JOIN STATS\n    stats = stats_main.join(stats_ast, on=[\"game_id\", \"personid\"], how=\"left\") \\\n                      .with_columns(pl.col(\"ast\").fill_null(0))\n\n    # -------------------------------------------------------------------------\n    # 5. UNI\u00d3N CON METADATA (Usando el game_opponents calculado al inicio)\n    # -------------------------------------------------------------------------\n    meta = df.group_by(\"game_id\").agg([\n        pl.col(\"season\").first().alias(\"season\"),\n        pl.col(\"game_date\").first().alias(\"game_date\")\n    ]).join(game_opponents, on=\"game_id\") # <--- Aqu\u00ed usamos la tabla calculada en Paso 0\n\n    final_df = stats.join(meta, on=\"game_id\")\n\n    # 6. C\u00c1LCULOS FINALES\n    final_df = final_df.with_columns([\n        (pl.col(\"fgm\") / pl.col(\"fga\")).fill_nan(0).alias(\"pct_fg\"),\n        (pl.col(\"ftm\") / pl.col(\"fta\")).fill_nan(0).alias(\"pct_ft\"),\n        (pl.col(\"3pm\") / pl.col(\"3pa\")).fill_nan(0).alias(\"pct_3\"),\n        pl.lit(0).alias(\"plus_minus\"),\n        \n        # L\u00f3gica de oponente\n        pl.struct([\"teamtricode\", \"teams\"]).map_elements(\n            lambda x: [t for t in x[\"teams\"] if t != x[\"teamtricode\"]][0] if len(x[\"teams\"]) > 1 else \"Unknown\",\n            return_dtype=pl.Utf8\n        ).alias(\"opponent\")\n    ])\n\n    # Renombres previos para encajar con el esquema\n    # Usamos alias seguros\n    final_df = final_df.with_columns([\n        pl.col(\"personid\").alias(\"player_id\"),\n        pl.col(\"playername\").alias(\"player_name\"),\n        pl.col(\"teamtricode\").alias(\"team\")\n    ])\n\n    # Garantizar cumplimiento estricto del DASHBOARD_GOLD_SCHEMA\n    \n    # 1. Crear columnas faltantes como Nulos\n    for col_name, dtype in DASHBOARD_GOLD_SCHEMA.items():\n        if col_name not in final_df.columns:\n            final_df = final_df.with_columns(pl.lit(None, dtype=dtype).alias(col_name))\n\n    # 2. Castear y Seleccionar\n    final_cols = []\n    for col_name, dtype in DASHBOARD_GOLD_SCHEMA.items():\n        final_cols.append(pl.col(col_name).cast(dtype, strict=False))\n\n    return final_df.select(final_cols).sort(\"pts\", descending=True).unique()\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    from default_repo.utils.schemas import validate_schema, DASHBOARD_GOLD_SCHEMA, DASHBOARD_GOLD_OUTPUT_CRITICAL\n    assert output is not None, 'The output is undefined'\n    \n    validate_schema(output, DASHBOARD_GOLD_SCHEMA, DASHBOARD_GOLD_OUTPUT_CRITICAL)", "file_path": "transformers/dashboard_pbp_gold.py", "language": "python", "type": "transformer", "uuid": "dashboard_pbp_gold"}, "transformers/nba_real_time_clean_gradio.py:transformer:python:nba real time clean gradio": {"content": "from typing import Dict, List\nimport requests\nimport json\nfrom datetime import datetime\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n# Cach\u00e9 para evitar peticiones repetitivas al CDN de la NBA\nCACHED_TEAMS = {}\n\ndef fetch_team_names(game_id: str) -> Dict:\n    \"\"\"Obtiene nombres y tricodes de los equipos usando el boxscore de la NBA.\"\"\"\n    if not game_id or game_id in CACHED_TEAMS:\n        return CACHED_TEAMS.get(game_id)\n    \n    url = f\"https://cdn.nba.com/static/json/liveData/boxscore/boxscore_{game_id}.json\"\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n    \n    try:\n        res = requests.get(url, headers=headers, timeout=5)\n        if res.status_code == 200:\n            game_data = res.json().get('game', {})\n            h = game_data.get('homeTeam', {})\n            a = game_data.get('awayTeam', {})\n            info = {\n                \"home_full\": f\"{h.get('teamCity')} {h.get('teamName')}\".strip() or \"Local\",\n                \"away_full\": f\"{a.get('teamCity')} {a.get('teamName')}\".strip() or \"Visitante\",\n                \"home_tri\": h.get('teamTricode', 'HOME'),\n                \"away_tri\": a.get('teamTricode', 'AWAY')\n            }\n            CACHED_TEAMS[game_id] = info\n            return info\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Error accediendo al CDN de la NBA ({game_id}): {e}\")\n    return None\n\ndef generar_prompt_nba(row):\n    # --- 1. Datos Universales ---\n    # Manejo de m\u00faltiples posibles nombres de columnas para el marcador\n    h_score = row.get('scoreHome') or row.get('homeScore', 0)\n    a_score = row.get('scoreAway') or row.get('awayScore', 0)\n    marcador = f\"{h_score}-{a_score}\"\n    \n    reloj = row.get('clock', '00:00')\n    periodo = row.get('period', 1)\n    jugador = row.get('playerName', 'Jugador')\n    equipo = row.get('teamTricode', 'N/A')\n    tipo = str(row.get('actionType', '')).lower()\n    subtipo = str(row.get('subType', '')).lower() if row.get('subType') else \"acci\u00f3n\"\n    \n    # --- 2. Construcci\u00f3n de la Ficha (Estructura Base) ---\n    ficha_dinamica = [\n        \"FICHA T\u00c9CNICA DE ACCI\u00d3N:\",\n        f\"- TIPO: {tipo.upper()} ({subtipo})\",\n        f\"- PROTAGONISTA: {jugador} ({equipo})\",\n        f\"- CONTEXTO: Cuarto {periodo} | Reloj: {reloj} | Marcador: {marcador}\"\n    ]\n\n    # --- 3. L\u00f3gica Segmentada por Tipo de Acci\u00f3n ---\n    \n    # TIROS (2pt, 3pt, shot)\n    if tipo in ['2pt', '3pt', 'shot']:\n        resultado = str(row.get('shotResult', '')).upper()\n        ficha_dinamica.append(f\"- RESULTADO: {resultado}\")\n        \n        if resultado == \"MADE\":\n            pts = 3 if '3pt' in tipo else 2\n            ficha_dinamica.append(f\"- PUNTOS EN LA ACCI\u00d3N: +{pts}\")\n            ficha_dinamica.append(f\"- PUNTOS TOTALES JUGADOR: {row.get('pointsTotal', 'N/A')}\")\n            if row.get('shotDistance'):\n                ficha_dinamica.append(f\"- DISTANCIA: {row.get('shotDistance')} pies\")\n            if row.get('assistPlayerNameInitial'):\n                ficha_dinamica.append(f\"- ASISTENCIA DE: {row.get('assistPlayerNameInitial')} (Total: {row.get('assistTotal', 0)})\")\n    \n    # TIROS LIBRES\n    elif tipo == 'freethrow':\n        resultado = str(row.get('shotResult', '')).upper()\n        ficha_dinamica.append(f\"- INTENTO: {subtipo}\")\n        ficha_dinamica.append(f\"- RESULTADO: {resultado}\")\n        if resultado == \"MADE\":\n            ficha_dinamica.append(f\"- PUNTOS TOTALES JUGADOR: {row.get('pointsTotal', 'N/A')}\")\n\n    # REBOTES\n    elif tipo == 'rebound':\n        ficha_dinamica.append(f\"- CLASE: {subtipo.upper()}\")\n        ficha_dinamica.append(f\"- TOTAL REBOTES JUGADOR: {row.get('reboundTotal', 'N/A')}\")\n\n    # SALTO INICIAL\n    elif tipo == 'jumpball':\n        ficha_dinamica.append(f\"- GANADOR: {row.get('jumpBallWonPlayerName', 'N/A')}\")\n        ficha_dinamica.append(f\"- PERDEDOR: {row.get('jumpBallLostPlayerName', 'N/A')}\")\n        ficha_dinamica.append(f\"- RECUPERADO POR: {row.get('jumpBallRecoveredName', 'N/A')}\")\n\n    # DEFENSIVOS (Block / Steal)\n    elif tipo in ['block', 'steal']:\n        actor = row.get('blockPlayerName') if tipo == 'block' else row.get('stealPlayerName')\n        ficha_dinamica.append(f\"- DEFENSOR: {actor}\")\n        ficha_dinamica.append(f\"- SOBRE: {jugador}\")\n\n    # FALTAS\n    elif tipo == 'foul':\n        ficha_dinamica.append(f\"- INFRACTOR: {jugador}\")\n        ficha_dinamica.append(f\"- RECIBE FALTA: {row.get('foulDrawnPlayerName', 'N/A')}\")\n        ficha_dinamica.append(f\"- FALTAS TOTALES: {row.get('foulPersonalTotal', 'N/A')}\")\n\n    # P\u00c9RDIDAS (Turnover)\n    elif tipo == 'turnover':\n        ficha_dinamica.append(f\"- CAUSA: {subtipo.upper()}\")\n        ficha_dinamica.append(f\"- P\u00c9RDIDAS TOTALES JUGADOR: {row.get('turnoverTotal', 'N/A')}\")\n\n    return \"\\n\".join(ficha_dinamica)\n\n@transformer\ndef transform(messages: List[Dict], *args, **kwargs):\n    transformed_batch = []\n\n    for msg in messages:\n        # Mage entrega: {'data': {JSON_KAFKA}, 'metadata': {CLAVES_KAFKA}}\n        data = msg.get('data', {})\n        meta = msg.get('metadata', {})\n\n        # EXTRAER GAME_ID: Fundamental para que el pipeline no se detenga\n        # Buscamos en data, meta o decodificamos la key de Kafka\n        raw_key = meta.get('key')\n        if isinstance(raw_key, bytes):\n            raw_key = raw_key.decode('utf-8')\n            \n        g_id = data.get('gameId') or data.get('game_id') or raw_key\n        \n        # Si no hay g_id, usamos el de tu log para evitar el 'continue'\n        if not g_id:\n            g_id = \"0022300650\"\n\n        # Obtener nombres reales de equipos\n        teams = fetch_team_names(str(g_id))\n        h_name = teams['home_full'] if teams else \"Equipo Local\"\n        a_name = teams['away_full'] if teams else \"Equipo Visitante\"\n\n        # Limpiar el formato del reloj (PT08M24.00S -> 08:24)\n        clock_raw = data.get('clock', '00:00')\n        clock = clock_raw\n        if \"PT\" in str(clock_raw):\n            try:\n                clock = clock_raw.replace(\"PT\", \"\").replace(\"M\", \":\").replace(\"S\", \"\")\n                p = clock.split(\":\")\n                clock = f\"{p[0].zfill(2)}:{p[1][:2]}\"\n            except:\n                pass\n            \n        period = data.get('period', \"0\")\n\n\n        # Construcci\u00f3n del evento para la capa Gold (Redpanda)\n        enriched_event = {\n            \"game_id\": str(g_id),\n            \"game_name\": f\"{h_name} vs {a_name}\",\n            \"score_display\": f\"{h_name} {data.get('scoreHome', 0)} - {data.get('scoreAway', 0)} {a_name}\",\n            \"clock\": clock,\n            \"period\": period,\n            \"description\": data.get('description', 'Acci\u00f3n de juego'),\n            \"ai_input\": generar_prompt_nba(data),\n            \"ingestion_time\": datetime.now().isoformat()\n        }\n\n        if enriched_event[\"description\"]:\n            transformed_batch.append(enriched_event)\n\n    print(f\"\u2705 Transformador completado: {len(transformed_batch)} mensajes procesados.\")\n    return transformed_batch\n", "file_path": "transformers/nba_real_time_clean_gradio.py", "language": "python", "type": "transformer", "uuid": "nba_real_time_clean_gradio"}, "pipelines/nba_pbp_realtime/metadata.yaml:pipeline:yaml:nba pbp realtime/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/nba_real_time_listener.yaml\n  downstream_blocks:\n  - nba_real_time_to_bronze\n  - nba_real_time_clean_gradio\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: nba_real_time_listener\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: nba_real_time_listener\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/nba_real_time_to_bronze.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: nba_real_time_to_bronze\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - nba_real_time_listener\n  uuid: nba_real_time_to_bronze\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: transformers/nba_real_time_clean_gradio.py\n  downstream_blocks:\n  - export_to_gradio\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: nba_real_time_clean_gradio\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - nba_real_time_listener\n  uuid: nba_real_time_clean_gradio\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/export_to_gradio.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_to_gradio\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - nba_real_time_clean_gradio\n  uuid: export_to_gradio\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-04 19:42:18.054369+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_pbp_realtime\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: streaming\nuuid: nba_pbp_realtime\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_pbp_realtime/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_pbp_realtime/metadata"}, "pipelines/nba_pbp_realtime/triggers.yaml:pipeline:yaml:nba pbp realtime/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2026-02-14 17:06:59.257310\n  name: always_run\n  pipeline_uuid: nba_pbp_realtime\n  schedule_interval: '@once'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2026-02-14 17:06:00\n  status: active\n  token: c2bf5f3ebbc94b33864f7a9ecb626bbd\n  variables: {}\n", "file_path": "pipelines/nba_pbp_realtime/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_pbp_realtime/triggers"}, "pipelines/nba_pbp_realtime/__init__.py:pipeline:python:nba pbp realtime/  init  ": {"content": "", "file_path": "pipelines/nba_pbp_realtime/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_pbp_realtime/__init__"}, "pipelines/nba_teams_master/metadata.yaml:pipeline:yaml:nba teams master/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/fetch_teams_raw.py\n    file_source:\n      path: data_loaders/fetch_teams_raw.py\n  downstream_blocks:\n  - extract_unique_teams\n  - save_teams_to_minio_bronze\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: fetch_teams_raw\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: fetch_teams_raw\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_teams_to_minio_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_unique_teams\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - fetch_teams_raw\n  uuid: extract_unique_teams\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_teams_to_minio_silver.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_teams_to_minio_silver\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_unique_teams\n  uuid: save_teams_to_minio_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_teams_to_minio_bronze.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_teams_to_minio_bronze\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - fetch_teams_raw\n  uuid: save_teams_to_minio_bronze\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-02 20:42:20.556560+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_teams_master\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: nba_teams_master\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_teams_master/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_teams_master/metadata"}, "pipelines/nba_teams_master/triggers.yaml:pipeline:yaml:nba teams master/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2026-02-03 19:59:29.401906\n  name: monthly_teams_check_01_1200_wet\n  pipeline_uuid: nba_teams_master\n  schedule_interval: '@monthly'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2026-03-02 12:00:00\n  status: active\n  token: 051f256494764665a29de9ec6828404a\n  variables: {}\n", "file_path": "pipelines/nba_teams_master/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_teams_master/triggers"}, "pipelines/nba_teams_master/__init__.py:pipeline:python:nba teams master/  init  ": {"content": "", "file_path": "pipelines/nba_teams_master/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_teams_master/__init__"}, "pipelines/nba_players_historical_backfill/metadata.yaml:pipeline:yaml:nba players historical backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_players_to_minio_bronze\n  - clean_players_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_historical_players\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_historical_players\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_players_to_minio_bronze.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_players_to_minio_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_historical_players\n  uuid: save_players_to_minio_bronze\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: transformers/clean_players_silver.py\n  downstream_blocks:\n  - save_players_to_minio_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: clean_players_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_historical_players\n  uuid: clean_players_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_players_to_minio_silver.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_players_to_minio_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - clean_players_silver\n  uuid: save_players_to_minio_silver\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-03 20:24:12.067467+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_players_historical_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: nba_players_historical_backfill\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_players_historical_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_players_historical_backfill/metadata"}, "pipelines/nba_players_historical_backfill/__init__.py:pipeline:python:nba players historical backfill/  init  ": {"content": "", "file_path": "pipelines/nba_players_historical_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_players_historical_backfill/__init__"}, "pipelines/nba_players_master/metadata.yaml:pipeline:yaml:nba players master/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_players_to_minio_bronze\n  - clean_players_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: fetch_players_raw\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: fetch_players_raw\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_players_to_minio_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - fetch_players_raw\n  uuid: save_players_to_minio_bronze\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_players_to_minio_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: clean_players_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - fetch_players_raw\n  uuid: clean_players_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_players_to_minio_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - clean_players_silver\n  uuid: save_players_to_minio_silver\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-02 20:26:50.257314+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_players_master\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: nba_players_master\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_players_master/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_players_master/metadata"}, "pipelines/nba_players_master/triggers.yaml:pipeline:yaml:nba players master/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2026-02-03 19:56:09.575679\n  name: weekly_players_sync_monday_1100_wet\n  pipeline_uuid: nba_players_master\n  schedule_interval: '@weekly'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2026-02-09 11:00:00\n  status: active\n  token: 7f2d51eb09cd4a3787cfdce2d1a3b7b0\n  variables: {}\n", "file_path": "pipelines/nba_players_master/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_players_master/triggers"}, "pipelines/nba_players_master/__init__.py:pipeline:python:nba players master/  init  ": {"content": "", "file_path": "pipelines/nba_players_master/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_players_master/__init__"}, "pipelines/nba_pbp_daily/metadata.yaml:pipeline:yaml:nba pbp daily/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - fetch_yesterday_games_pbp\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: updated_schedule\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: updated_schedule\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_pbp_to_minio_bronze\n  - clean_pbp_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: fetch_yesterday_games_pbp\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - updated_schedule\n  uuid: fetch_yesterday_games_pbp\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_pbp_to_minio_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - fetch_yesterday_games_pbp\n  uuid: save_pbp_to_minio_bronze\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_pbp_to_mino_silver\n  - dashboard_pbp_gold\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: clean_pbp_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - fetch_yesterday_games_pbp\n  uuid: clean_pbp_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_pbp_to_mino_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - clean_pbp_silver\n  uuid: save_pbp_to_mino_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - save_pbp_to_minio_gold\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dashboard_pbp_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - clean_pbp_silver\n  uuid: dashboard_pbp_gold\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_pbp_to_minio_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - dashboard_pbp_gold\n  uuid: save_pbp_to_minio_gold\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-02 20:32:00.710845+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_pbp_daily\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: nba_pbp_daily\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_pbp_daily/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_pbp_daily/metadata"}, "pipelines/nba_pbp_daily/triggers.yaml:pipeline:yaml:nba pbp daily/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2026-02-03 19:55:08.428832\n  name: daily_pbp_extraction_1000_wet\n  pipeline_uuid: nba_pbp_historical\n  schedule_interval: '@daily'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2026-02-04 10:00:00\n  status: active\n  token: d2f9584a48ab4e0fab9cd401da517d87\n  variables: {}\n", "file_path": "pipelines/nba_pbp_daily/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_pbp_daily/triggers"}, "pipelines/nba_pbp_daily/__init__.py:pipeline:python:nba pbp daily/  init  ": {"content": "", "file_path": "pipelines/nba_pbp_daily/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_pbp_daily/__init__"}, "pipelines/nba_teams_historical_backfill_copy/metadata.yaml:pipeline:yaml:nba teams historical backfill copy/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/load_historical_teams.py\n  downstream_blocks:\n  - extract_unique_teams\n  - save_teams_to_minio_bronze\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_historical_teams\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_historical_teams\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_teams_to_minio_bronze.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_teams_to_minio_bronze\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_historical_teams\n  uuid: save_teams_to_minio_bronze\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: transformers/extract_unique_teams.py\n  downstream_blocks:\n  - save_teams_to_minio_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_unique_teams\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_historical_teams\n  uuid: extract_unique_teams\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_teams_to_minio_silver.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_teams_to_minio_silver\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_unique_teams\n  uuid: save_teams_to_minio_silver\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-03 20:24:12.067467+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_teams_historical_backfill_copy\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: nba_teams_historical_backfill_copy\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_teams_historical_backfill_copy/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_teams_historical_backfill_copy/metadata"}, "pipelines/nba_teams_historical_backfill_copy/__init__.py:pipeline:python:nba teams historical backfill copy/  init  ": {"content": "", "file_path": "pipelines/nba_teams_historical_backfill_copy/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_teams_historical_backfill_copy/__init__"}, "pipelines/nba_pbp_historical_backfill/metadata.yaml:pipeline:yaml:nba pbp historical backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/load_historical_games_ids.py\n    file_source:\n      path: data_loaders/load_historical_games_ids.py\n  downstream_blocks:\n  - pbp_download_historical\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_historical_games_ids\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_historical_games_ids\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/pbp_download_historical.py\n    file_source:\n      path: data_loaders/pbp_download_historical.py\n  downstream_blocks:\n  - save_pbp_to_minio_bronze\n  - clean_pbp_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: pbp_download_historical\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - load_historical_games_ids\n  uuid: pbp_download_historical\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_pbp_to_minio_bronze.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_pbp_to_minio_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - pbp_download_historical\n  uuid: save_pbp_to_minio_bronze\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: transformers/clean_pbp_silver.py\n  downstream_blocks:\n  - save_pbp_to_mino_silver\n  - dashboard_pbp_gold\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: clean_pbp_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - pbp_download_historical\n  uuid: clean_pbp_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_pbp_to_mino_silver.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_pbp_to_mino_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - clean_pbp_silver\n  uuid: save_pbp_to_mino_silver\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: transformers/dashboard_pbp_gold.py\n  downstream_blocks:\n  - save_pbp_to_minio_gold\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dashboard_pbp_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - clean_pbp_silver\n  uuid: dashboard_pbp_gold\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_exporters/save_pbp_to_minio_gold.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_pbp_to_minio_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - dashboard_pbp_gold\n  uuid: save_pbp_to_minio_gold\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-03 20:24:12.067467+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: nba_pbp_historical_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: nba_pbp_historical_backfill\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/nba_pbp_historical_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "nba_pbp_historical_backfill/metadata"}, "pipelines/nba_pbp_historical_backfill/__init__.py:pipeline:python:nba pbp historical backfill/  init  ": {"content": "", "file_path": "pipelines/nba_pbp_historical_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "nba_pbp_historical_backfill/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}